<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="generator" content="Hexo 4.2.0"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>关于爬虫自己所了解的功能梳理 - 孙军雨的技术博客</title><meta description="一.基础1.虚拟环境Python 123456789101112131. 虚拟环境就是隔离的空间       1. virtualenvwrapper 管理 虚拟环境       2. pip install virtualenvwrapper-win -i 网址       3. virtual 虚拟       4. env 环境       5. wrapper2. mkvirtualenv"><meta property="og:type" content="article"><meta property="og:title" content="关于爬虫自己所了解的功能梳理"><meta property="og:url" content="https://sun-junyu.github.io/posts/468ef90b/"><meta property="og:site_name" content="孙军雨的技术博客"><meta property="og:description" content="一.基础1.虚拟环境Python 123456789101112131. 虚拟环境就是隔离的空间       1. virtualenvwrapper 管理 虚拟环境       2. pip install virtualenvwrapper-win -i 网址       3. virtual 虚拟       4. env 环境       5. wrapper2. mkvirtualenv"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://q92ekyelb.bkt.clouddn.com/xsy.jpg"><meta property="article:published_time" content="2020-03-06T16:00:00.000Z"><meta property="article:modified_time" content="2020-04-22T05:52:53.356Z"><meta property="article:author" content="removeif"><meta property="article:tag" content="爬虫知识点梳理"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="http://q92ekyelb.bkt.clouddn.com/xsy.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://sun-junyu.github.io/posts/468ef90b/"},"headline":"关于爬虫自己所了解的功能梳理","image":["http://q92ekyelb.bkt.clouddn.com/xsy.jpg"],"datePublished":"2020-03-06T16:00:00.000Z","dateModified":"2020-04-22T05:52:53.356Z","author":{"@type":"Person","name":"removeif"},"description":"一.基础1.虚拟环境Python 123456789101112131. 虚拟环境就是隔离的空间       1. virtualenvwrapper 管理 虚拟环境       2. pip install virtualenvwrapper-win -i 网址       3. virtual 虚拟       4. env 环境       5. wrapper2. mkvirtualenv"}</script><link rel="alternative" href="/atom.xml" title="孙军雨的技术博客" type="application/atom+xml"><link rel="icon" href="http://q92ekyelb.bkt.clouddn.com/authon.jpg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:400,600|Source+Code+Pro|Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Microsoft YaHei:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&amp;amp;subset=latin,latin-ext|Inconsolata|Itim|Lobster.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-dark.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/removeif/removeif.github.io@v2.2.1/css/style.css"><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/gh/removeif/removeif.github.io@v2.2.1/js/globalUtils.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/removeif/removeif.github.io@v2.2.1/live2d/waifu.css"><script type="text/javascript" async src="https://cdn.jsdelivr.net/gh/removeif/removeif.github.io@v2.2.1/live2d/autoload.js"></script></head><body class="is-3-column has-navbar-fixed-top"><nav class="navbar navbar-main is-fixed-top"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="http://q92ekyelb.bkt.clouddn.com/authon.jpg" alt="孙军雨的技术博客" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">首页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/tags/%E6%97%A5%E8%AE%B0/">日记</a><a class="navbar-item" href="/media">影音</a><a class="navbar-item" href="/album">相册</a><a class="navbar-item" href="/message">留言</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Sun-JunYu"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a><a class="navbar-item" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a></div></div></div></nav><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/removeif/removeif.github.io@v2.2.1/js/theme-setting.js"></script><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="thumbnail" src="https://cdn.jsdelivr.net/gh/removeif/blog_image/img/2019/20191221090606.png" alt="关于爬虫自己所了解的功能梳理"></span></div><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-03-06T16:00:00.000Z">2020-03-07</time><a class="commentCountImg" href="/posts/468ef90b/#comment-container"><span class="display-none-class">d1d93ece00a6aa09046fd549769ddff2</span><img class="not-gallery-item" src="https://cdn.jsdelivr.net/gh/removeif/removeif.github.io@v2.2.1/img/chat.svg"> <span class="commentCount" id="d1d93ece00a6aa09046fd549769ddff2"> 99+</span>    </a><span class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a></span><span class="level-item">1 小时 读完 (大约 8193 个字)</span><span class="level-item" id="busuanzi_container_page_pv"><i class="far fa-eye"></i>&nbsp;&nbsp;<span id="busuanzi_value_page_pv">0</span>次访问</span></div></div><h1 class="title is-3 is-size-4-mobile">关于爬虫自己所了解的功能梳理</h1><div class="content"><h1 id="一-基础"><a href="#一-基础" class="headerlink" title="一.基础"></a>一.基础</h1><h2 id="1-虚拟环境"><a href="#1-虚拟环境" class="headerlink" title="1.虚拟环境"></a>1.虚拟环境</h2><p>Python</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">1. 虚拟环境就是隔离的空间</span><br><span class="line">       1. virtualenvwrapper 管理 虚拟环境</span><br><span class="line">       2. pip install virtualenvwrapper-win -i 网址</span><br><span class="line">       3. virtual 虚拟</span><br><span class="line">       4. env 环境</span><br><span class="line">       5. wrapper</span><br><span class="line">2. mkvirtualenv envname(虚拟环境名)  # 创建虚拟环境并自动切换到该环下</span><br><span class="line">3. workon 虚拟环境名  激活虚拟环境</span><br><span class="line">4. deactivate 退出虚拟环境</span><br><span class="line">5. lsvirtualenv 查看所有的虚拟环境</span><br><span class="line">6. rmvirtualenv 名字 删除虚拟环境</span><br><span class="line">7. pip freeze &gt; requirements.txt  #安装 pip install -r  requirements.txt</span><br><span class="line">8. mkvirtualenv normal -p D:\Python3.8\python.exe</span><br></pre></td></tr></table></figure>

<a id="more"></a>

<h2 id="2-什么是爬虫"><a href="#2-什么是爬虫" class="headerlink" title="2.什么是爬虫"></a>2.什么是爬虫</h2><p>网络爬虫（又被称为网页蜘蛛，网络机器人）就是 模拟客户端(主要指浏览器)发送网络请求，接收请求响应，按照一定的规则，自动地抓取互联网信息的程 序。原则上,只要是客户端(主要指浏览器)能做的事情，爬虫都能够做</p>
<h2 id="3-爬虫的用途"><a href="#3-爬虫的用途" class="headerlink" title="3.爬虫的用途"></a>3.爬虫的用途</h2><ul>
<li><p>数据采集</p>
</li>
<li><p>软件测试</p>
</li>
<li><p>爬虫之自动化测试</p>
</li>
<li><p>12306抢票</p>
</li>
<li><p>网站上的投票</p>
</li>
<li><p>投票网</p>
</li>
<li><p>短信轰炸</p>
</li>
<li><p>注册页面</p>
</li>
<li><p>web漏洞扫</p>
<h2 id="4-爬虫数据的来源"><a href="#4-爬虫数据的来源" class="headerlink" title="4.爬虫数据的来源"></a>4.爬虫数据的来源</h2></li>
<li><p>去第三方的公司购买数据(比如企查查)</p>
</li>
<li><p>去免费的数据网站下载数据(比如国家统计局)</p>
</li>
<li><p>通过爬虫爬取数据</p>
</li>
<li><p>人工收集数据(比如问卷调查)</p>
</li>
<li><p>爬虫的概念：模拟浏览器发送网络请求，接收请求响应</p>
</li>
</ul>
<h2 id="5-爬虫的分类"><a href="#5-爬虫的分类" class="headerlink" title="5.爬虫的分类"></a>5.爬虫的分类</h2><ul>
<li>通用爬虫 ：通常指搜索引擎的爬虫（<a href="https://www.baidu.com%29/">https://www.baidu.com）</a></li>
</ul>
<ul>
<li><p>聚焦爬虫 ：针对特定网站的爬虫</p>
<p> 针对特定领域</p>
<p> 抓取特定数据</p>
<p> 设计思路:</p>
<pre><code>1. 确定URL，向服务器发起请求，获取响应
2. 数据解析—&gt; 目标数据
3. 持久化到本地
 4. 金融量化分析/对冲
 1. 给机器学习的模型提供训练数据</code></pre></li>
<li><p>一般是以上两种</p>
<ul>
<li>增量式爬虫 ：只爬取新产生的或者已经发生变化网页的爬虫</li>
<li>深网爬虫 ：隐藏在搜索表单或登录表单之后的数据，只有用户提交一些关键词 或登录才能获得的 Web 页面</li>
</ul>
</li>
</ul>
<p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/5c262ead1068a1f1b61a7a867c70e7c.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/5c262ead1068a1f1b61a7a867c70e7c.png" alt="5c262ead1068a1f1b61a7a867c70e7c"></a></p>
<p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/46d8f00e26e814f4ea7690ee39e5907.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/46d8f00e26e814f4ea7690ee39e5907.png" alt="46d8f00e26e814f4ea7690ee39e5907"></a></p>
<h2 id="6-B-S架构和C-S架构"><a href="#6-B-S架构和C-S架构" class="headerlink" title="6.B/S架构和C/S架构"></a>6.B/S架构和C/S架构</h2><ul>
<li>B/S架构 B-&gt; Browser 浏览器 , S-&gt;Server 服务器</li>
<li>C/S架构 C-&gt;client 客户端, S-&gt;Server</li>
<li>B/S架构是特殊的C/S架构</li>
</ul>
<h2 id="7-PHP不适合做爬虫"><a href="#7-PHP不适合做爬虫" class="headerlink" title="7.PHP不适合做爬虫"></a>7.PHP不适合做爬虫</h2><ul>
<li>因为数据时效性比较差</li>
<li>时效性比较差比太适合爬虫</li>
</ul>
<h2 id="8-爬虫语言"><a href="#8-爬虫语言" class="headerlink" title="8.爬虫语言"></a>8.爬虫语言</h2><p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/f8247a2132cb26c00d0171d7cfced04.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/f8247a2132cb26c00d0171d7cfced04.png" alt="f8247a2132cb26c00d0171d7cfced04"></a></p>
<ul>
<li>PHP:并发能力差，对多进程和多线程支持不好，数据量较大时爬虫效率较低</li>
<li>C/C++:语言效率高，但是学习成本较大，对程序员的技术能力要求较高，目前还停留在研究阶段，市场需求低</li>
<li>Java:python爬虫的主要竞争对手，由于java的特点，代码臃肿，代码量大，维护成本高，开发效率低，但是目前市场上市场上需求量还是挺高</li>
<li>Python:语法简单，学习成本较低，对新手比较友好，Python语言良好的生态，大量和框架的支持是的python爬虫目前处于主导地位</li>
</ul>
<h2 id="9-robots协议"><a href="#9-robots协议" class="headerlink" title="9.robots协议"></a>9.robots协议</h2><ul>
<li>Robots协议（也称为爬虫协议、机器人协议等）的全称是“网络爬虫排除标准”（Robots Exclusion Protocol），网站通过Robots协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取。</li>
<li>Robots协议是一个道德层面的约定，爬虫作者尊不遵守完全看自己意愿。</li>
</ul>
<h2 id="10-网络协议"><a href="#10-网络协议" class="headerlink" title="10.网络协议"></a>10.网络协议</h2><ol>
<li><p>OSI七层:</p>
<ul>
<li>应用层</li>
<li>会话层</li>
<li>传输层</li>
<li>网络层</li>
<li>数据链路层</li>
<li>物理层</li>
</ul>
</li>
<li><p>TCP/IP五层</p>
<ul>
<li>应用层 http/https,FTP文件传输协议</li>
<li>传输层 TCP/UDP</li>
<li>网络层 IP</li>
<li>数据链路层 ARP</li>
<li>物理层 以太网</li>
</ul>
</li>
<li><p>HTT[协议和HTTPS</p>
<p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/1576656784(1).png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/1576656784(1).png" alt="1576656784(1)"></a></p>
</li>
<li><p>TCP和UDP</p>
<p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/1576657100(1).jpg"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/1576657100(1).jpg" alt="1576657100(1)"></a></p>
<ul>
<li>TCP面向链接的 基字节的流式传输，可靠<ul>
<li>有序性</li>
<li>正确性</li>
<li>可靠性</li>
<li>可控性</li>
</ul>
</li>
<li>UDP协议：面向无连接，用户数据报，不可靠<ul>
<li>无连接，数据可能丢失或者循坏</li>
<li>报文小，传输效率高</li>
<li>吞吐量大的网络传输，可能在一定程度上承受数据丢失</li>
</ul>
</li>
</ul>
</li>
<li><p>服务器常见的端口</p>
<p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/1576659512(1).png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/1576659512(1).png" alt="1576659512(1)"></a></p>
<ol>
<li>ftp:File Transfer Protocol 的缩写,即文件传输协议,端口 :21</li>
<li>ssh:Secure Shell的缩写，用于远程登陆会话，端口:22</li>
<li>MySQL：关系型数据库 ,端口:3306</li>
<li>MangoDB:非关系型数据库,端口27017</li>
<li>Redis:非关系型数据库,端口:6379</li>
</ol>
</li>
</ol>
<h2 id="11-第一次请求"><a href="#11-第一次请求" class="headerlink" title="11. 第一次请求"></a>11. 第一次请求</h2><ol>
<li><p>URL介绍</p>
<ul>
<li><p>URL作用式用于定位服务器资源的</p>
<p>请求过程:</p>
<ul>
<li>客户端，通常指WEB浏览器或APP向服务器发起请求，服务器接收到请求进行处理，并向客户端发起响应</li>
</ul>
</li>
</ul>
</li>
<li><p>请求方法</p>
<p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/cb16878f262d9ba5cd0163514cf0577.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/cb16878f262d9ba5cd0163514cf0577.png" alt="cb16878f262d9ba5cd0163514cf0577"></a></p>
<p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/1576723105(1).jpg"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/1576723105(1).jpg" alt="1576723105(1)"></a></p>
</li>
<li><p>请求头</p>
<p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/1576723785(1).jpg"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/1576723785(1).jpg" alt="1576723785(1)"></a></p>
</li>
<li><p>请求体</p>
<p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/46cf8b931131d8e00b5dd4e78418238.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/46cf8b931131d8e00b5dd4e78418238.png" alt="46cf8b931131d8e00b5dd4e78418238"></a></p>
</li>
<li><p>状态码</p>
<p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/F8O$8G8YV_%5D~J_R@MO5T%7BF9.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/F8O$8G8YV_%5D~J_R@MO5T%7BF9.png" alt="img"></a></p>
</li>
</ol>
<ol>
<li><p>响应体</p>
<p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/f67f35fa5e80bfe3b5e260a5d305b0e.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/f67f35fa5e80bfe3b5e260a5d305b0e.png" alt="f67f35fa5e80bfe3b5e260a5d305b0e"></a></p>
</li>
<li><p>网页基础</p>
<p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/1576725709(1).jpg"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/1576725709(1).jpg" alt="1576725709(1)"></a></p>
</li>
<li><p>GET请求</p>
<ul>
<li>url : 请求的链接 是一个字符串</li>
<li>headers : 请求头是一个字典</li>
<li>params:请求参数，字典</li>
<li>verify:禁止证书验证，SSLError</li>
</ul>
</li>
<li><p>POST请求</p>
<ul>
<li>url</li>
<li>headers</li>
<li>data</li>
<li>verify</li>
</ul>
</li>
</ol>
<h2 id="12-JSON爬取和JSON的一些简单的解析规则"><a href="#12-JSON爬取和JSON的一些简单的解析规则" class="headerlink" title="12.JSON爬取和JSON的一些简单的解析规则"></a>12.JSON爬取和JSON的一些简单的解析规则</h2><p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191226140526.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191226140526.png" alt="QQ截图20191226140526"></a></p>
<p>Python</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">import csv</span><br><span class="line"></span><br><span class="line">from lxml import etree</span><br><span class="line">import requests</span><br><span class="line">import json</span><br><span class="line">import re</span><br><span class="line"></span><br><span class="line">with open(&#39;data.csv&#39;, &#39;w&#39;, encoding&#x3D;&#39;utf-8&#39;) as f:</span><br><span class="line">    writer &#x3D; csv.writer(f, delimiter&#x3D;&#39;,&#39;)</span><br><span class="line">    writer.writerow([&#39;标题&#39;, &#39;内容&#39;, &#39;图片&#39;, &#39;时间&#39;])</span><br><span class="line"></span><br><span class="line">for i in range(1,5):</span><br><span class="line">    url &#x3D; &#39;http:&#x2F;&#x2F;news.cctv.com&#x2F;2019&#x2F;07&#x2F;gaiban&#x2F;cmsdatainterface&#x2F;page&#x2F;china_%s.jsonp?cb&#x3D;t&amp;cb&#x3D;china&#39;%i</span><br><span class="line"></span><br><span class="line">    headers &#x3D; &#123;</span><br><span class="line">        &#39;User-Agent&#39; : &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;79.0.3945.88 Safari&#x2F;537.36&#39;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ret &#x3D; requests.get(url&#x3D;url,headers&#x3D;headers)</span><br><span class="line">    ret.encoding &#x3D; &#39;utf-8&#39;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    res &#x3D; re.findall(r&#39;\&#123;.*\&#125;&#39;,ret.text)[0]</span><br><span class="line">    ret &#x3D; json.loads(res)[&#39;data&#39;][&#39;list&#39;]</span><br><span class="line">    for i in ret:</span><br><span class="line">        image &#x3D; i[&#39;image2&#39;]</span><br><span class="line">        title &#x3D; i[&#39;title&#39;]</span><br><span class="line">        brief &#x3D; i[&#39;brief&#39;]</span><br><span class="line">        focus_date &#x3D; i[&#39;focus_date&#39;]</span><br><span class="line">        with open(&#39;data.csv&#39;, &#39;a&#39;, encoding&#x3D;&#39;gbk&#39;) as f:</span><br><span class="line">            writer &#x3D; csv.writer(f, delimiter&#x3D;&#39;,&#39;)</span><br><span class="line">            writer.writerow([title,brief,image,focus_date])</span><br></pre></td></tr></table></figure>

<h1 id="二-爬虫的准备工作和爬虫的一些基础"><a href="#二-爬虫的准备工作和爬虫的一些基础" class="headerlink" title="二.爬虫的准备工作和爬虫的一些基础"></a>二.爬虫的准备工作和爬虫的一些基础</h1><h2 id="1-安装"><a href="#1-安装" class="headerlink" title="1. 安装"></a>1. 安装</h2><ol>
<li>在指定环境下安装 pip install requests包(在需要爬虫的虚拟环境等安装此包)</li>
<li><a href="https://blog.csdn.net/aaronjny/article/details/77945329">https://blog.csdn.net/aaronjny/article/details/77945329</a> （博客）</li>
</ol>
<h2 id="2-第一个简单的爬虫"><a href="#2-第一个简单的爬虫" class="headerlink" title="2.第一个简单的爬虫"></a>2.第一个简单的爬虫</h2><p>Python</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">bd &#x3D; &#39;http:&#x2F;&#x2F;www.baidu.com&#39;</span><br><span class="line">sg &#x3D; &#39;http:&#x2F;&#x2F;www.sogou.com&#39;</span><br><span class="line">res &#x3D; requests.get(url&#x3D;qc)  # 请求页面</span><br><span class="line"></span><br><span class="line"># print(res)  # 打印请求结果的状态码</span><br><span class="line"># print(res.text)  # 打印请求到的网页的源码</span><br><span class="line"># print(res.content) # 二进制的网页源码</span><br><span class="line"></span><br><span class="line">res.encoding &#x3D; &#39;urt-8&#39;  # 根据网页的编码格式更改</span><br><span class="line"></span><br><span class="line">with open(&#39;qianchengwuyou.html&#39;, &#39;w&#39;, encoding&#x3D;&#39;utf-8&#39;) as f:</span><br><span class="line">    f.write(res.text)</span><br><span class="line">    </span><br><span class="line">with open(&#39;qianchengwuyou.html&#39;, &#39;wb&#39;) as f:  # 以二进制写入，就不存在乱码 一般不推荐使用</span><br><span class="line">    f.write(res.content)</span><br></pre></td></tr></table></figure>

<h2 id="3-爬虫的一些语法"><a href="#3-爬虫的一些语法" class="headerlink" title="3.爬虫的一些语法"></a>3.爬虫的一些语法</h2><ol>
<li><p>爬虫的一些发送请求</p>
<p><a href="file:///C:/%5CUsers%5CXiaobai%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20191219164309237.png"><img src="file:///C:/%5CUsers%5CXiaobai%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20191219164309237.png" alt="image-20191219164309237"></a>)!(/my_technology_blog_hexo/markdown_img/4cb3f14a08714f863659a7325ec7335.png)</p>
</li>
</ol>
<h2 id="5-requests的高级应用"><a href="#5-requests的高级应用" class="headerlink" title="5.requests的高级应用"></a>5.requests的高级应用</h2><ol>
<li><p>上传文件</p>
<p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191220103658.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191220103658.png" alt="QQ截图20191220103658"></a></p>
</li>
<li><p>Cookie维持</p>
<p> <a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191220104533.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191220104533.png" alt="QQ截图20191220104533"></a></p>
<p>​</p>
<p>Python</p>
<p>​</p>
<p>​</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#39;Cookie&#39; : &#39;网页登陆信息的cookie的值&#39;</span><br></pre></td></tr></table></figure>


</li>
</ol>
<ol>
<li><p>Session状态维持</p>
<p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191220110431.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191220110431.png" alt="QQ截图20191220110431"></a></p>
</li>
<li><p>SLL证书验证</p>
<p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191220113255.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191220113255.png" alt="QQ截图20191220113255"></a>)<a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191220113334.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191220113334.png" alt="QQ截图20191220113334"></a></p>
</li>
<li><p>代理IP数据构建</p>
<p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191220113942.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191220113942.png" alt="QQ截图20191220113942"></a>)<a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191220115123.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191220115123.png" alt="QQ截图20191220115123"></a></p>
</li>
<li><p>超时设置</p>
<p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191220154603.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191220154603.png" alt="QQ截图20191220154603"></a></p>
<p>​</p>
<p>Python</p>
<p>​</p>
<p>​</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line">url &#x3D; &#39;https:&#x2F;&#x2F;www.baidu.com&#39;</span><br><span class="line"></span><br><span class="line">headers &#x3D; &#123;</span><br><span class="line">    &#39;User-Agent&#39; : &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;79.0.3945.88 Safari&#x2F;537.36&#39;,</span><br><span class="line">&#125;</span><br><span class="line">try:</span><br><span class="line">    res &#x3D; requests.get(url&#x3D;url,headers&#x3D;headers,timeout&#x3D;0.01)</span><br><span class="line">except:</span><br><span class="line">    print(&#39;超时&#39;)</span><br></pre></td></tr></table></figure>


</li>
</ol>
<ol>
<li><p>构建Request对象</p>
<p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/%E6%9E%84%E5%BB%BARequest.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/%E6%9E%84%E5%BB%BARequest.png" alt="构建Request"></a>)<a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/%E6%9E%84%E5%BB%BARequ2.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/%E6%9E%84%E5%BB%BARequ2.png" alt="构建Requ2"></a></p>
</li>
<li><p>urllib简单介绍</p>
<p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/urllib.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/urllib.png" alt="urllib"></a></p>
<p>![urllib get](/my_technology_blog_hexo/markdown_img/urllib get.png)</p>
<p>![urllib post](/my_technology_blog_hexo/markdown_img/urllib post.png)</p>
</li>
<li><p>re模块</p>
<p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/re%E6%A8%A1%E5%9D%97.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/re%E6%A8%A1%E5%9D%97.png" alt="re模块"></a></p>
</li>
</ol>
<h2 id="6-利用urllib-request中的urlretrieve爬取图片"><a href="#6-利用urllib-request中的urlretrieve爬取图片" class="headerlink" title="6.利用urllib.request中的urlretrieve爬取图片"></a>6.利用urllib.request中的urlretrieve爬取图片</h2><p>Python</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">from urllib.request import urlretrieve</span><br><span class="line">import re</span><br><span class="line">import json</span><br><span class="line">headers &#x3D; &#123;</span><br><span class="line">    &#39;User-Agent&#39; : &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;79.0.3945.88 Safari&#x2F;537.36&#39;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">url &#x3D; &#39;https:&#x2F;&#x2F;tu.enterdesk.com&#x2F;&#39;</span><br><span class="line"></span><br><span class="line">res &#x3D; requests.get(url&#x3D;url,headers&#x3D;headers)</span><br><span class="line">src_list &#x3D; re.findall(r&#39;https:&#x2F;&#x2F;up.enterdesk.com&#x2F;.*?\.jpg&#39;,res.text)</span><br><span class="line">for src in src_list:</span><br><span class="line">    name &#x3D; src.split(&#39;&#x2F;&#39;)[-1]</span><br><span class="line">    urlretrieve(src,&#39;.&#x2F;imgs&#x2F;%s&#39;%name)</span><br></pre></td></tr></table></figure>

<h2 id="7-正则"><a href="#7-正则" class="headerlink" title="7.正则"></a>7.正则</h2><ol>
<li>分组</li>
</ol>
<p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/%E5%88%86%E7%BB%84%E5%8C%B9%E9%85%8D.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/%E5%88%86%E7%BB%84%E5%8C%B9%E9%85%8D.png" alt="分组匹配"></a></p>
<ol>
<li>分组</li>
</ol>
<p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%BC%8F.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%BC%8F.png" alt="匹配模式"></a></p>
<ol>
<li><p>re模块</p>
<p><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191221094157.png" alt="QQ截图20191221094157"></p>
</li>
</ol>
<h2 id="8-xpath的语法"><a href="#8-xpath的语法" class="headerlink" title="8.xpath的语法"></a>8.xpath的语法</h2><ol>
<li>xpath解析库 介绍</li>
</ol>
<p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191221103812.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191221103812.png" alt="QQ截图20191221103812"></a></p>
<ol>
<li><p>xpath安装 初体验–&gt;使用步骤</p>
<p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191221104953.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191221104953.png" alt="QQ截图20191221104953"></a></p>
</li>
<li><p>常规语法</p>
</li>
</ol>
<p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191221105854.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191221105854.png" alt="QQ截图20191221105854"></a></p>
<p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191221105909.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191221105909.png" alt="QQ截图20191221105909"></a></p>
<p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191221110337.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191221110337.png" alt="QQ截图20191221110337"></a></p>
<h2 id="9-BeautifulSoup库使用"><a href="#9-BeautifulSoup库使用" class="headerlink" title="9.BeautifulSoup库使用"></a>9.BeautifulSoup库使用</h2><p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191223092602.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191223092602.png" alt="QQ截图20191223092602"></a></p>
<p>Python</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">soup &#x3D; BeautifulSoup(res.text,&#39;lxml&#39;)  # 必须装好lxml</span><br><span class="line"></span><br><span class="line">tab &#x3D; soup.find_all(name,atters,text)  # 方法选择器</span><br></pre></td></tr></table></figure>

<p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191223092902.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/" alt="QQ截图20191223092902"></a></p>
<ol>
<li><p>方法选择器</p>
<p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191224083721.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/" alt="QQ截图20191224083721"></a></p>
</li>
<li><p>类选择器</p>
<p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191224084005.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/" alt="QQ截图20191224084005"></a></p>
</li>
<li><p>嵌套选择器</p>
<p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191224084123.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/" alt="QQ截图20191224084123"></a></p>
<ol>
<li>获取文本和属性<ul>
<li>string : 获取直接子文本,如果 有平行标签的话，返回的是一个None</li>
<li>get_text() : 获取的是子孙节点的所有文本</li>
<li>element[‘attribute’] : 节点的属性</li>
</ul>
</li>
</ol>
</li>
</ol>
<h2 id="10-CSV写入文件和JSON写入"><a href="#10-CSV写入文件和JSON写入" class="headerlink" title="10.CSV写入文件和JSON写入"></a>10.CSV写入文件和JSON写入</h2><ul>
<li><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191223152022.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/" alt="QQ截图20191223152022"></a></li>
<li><code>import csv
with open(&#39;data.csv&#39;,&#39;w&#39;,encoding=&#39;utf-8&#39;) as csvf:
​    writer = csv.writer(csvf,delimiter=&#39;,&#39;)
​    writer.writerow([&#39;标题&#39;,&#39;内容&#39;,&#39;姓名&#39;]) 
\#这是 csv 的    
**Code**- ```python  import json  lst = []  for i in range(100):      d = {&#39;name&#39;: &#39;%s名字&#39;%i, &#39;age&#39;: i*3}      lst.append(d)  with open(&#39;data.json&#39;, &#39;w&#39;, encoding=&#39;utf-8&#39;) as f:      f.write(json.dumps(lst, ensure_ascii=False, indent=4))  # 这是 json的</code></li>
</ul>
<h2 id="11-selenium介绍"><a href="#11-selenium介绍" class="headerlink" title="11.selenium介绍"></a>11.selenium介绍</h2><ol>
<li><p>介绍</p>
<p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191224085324.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/" alt="QQ截图20191224085324"></a></p>
</li>
<li><p>安装</p>
<p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191224085634.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/" alt="QQ截图20191224085634"></a></p>
<ul>
<li>推荐使用谷歌浏览器稳定版</li>
<li>三要素: 浏览器，驱动程序，selenium框架</li>
<li>驱动下载地址:<a href="http://chromedriver.storage.googleapis.com/index.html">http://chromedriver.storage.googleapis.com/index.html</a></li>
<li>安装 pip install selenium</li>
</ul>
<p>​</p>
<p>Python</p>
<p>​</p>
<p>​</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#测试</span><br><span class="line">from selenium import webdriver</span><br><span class="line"></span><br><span class="line">browser &#x3D; webdriver.Chrome(&#39;.&#x2F;chromedriver.exe&#39;)</span><br><span class="line">browser.get(&#39;https:&#x2F;&#x2F;www.baidu.com&#39;)</span><br></pre></td></tr></table></figure>


</li>
</ol>
<ol>
<li><p>常用操作</p>
<p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191224090258.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191224090258.png" alt="QQ截图20191224090258"></a></p>
</li>
<li><p>获取页面元素</p>
<p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191224093114.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191224093114.png" alt="QQ截图20191224093114"></a></p>
</li>
<li><p>交互操作</p>
<p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191224093702.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191224093702.png" alt="QQ截图20191224093702"></a></p>
</li>
<li><p>获取网页数据</p>
<p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191224093848.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/" alt="QQ截图20191224093848"></a></p>
</li>
</ol>
<p>Python</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">js&#x3D;&#39;window.scrollTo(0,document.body.scrollHeight)&#39;</span><br><span class="line">brower.execute_script(js)</span><br><span class="line"># 滚动到最底下</span><br></pre></td></tr></table></figure>

<p>Python</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">html &#x3D; brower.page_source</span><br><span class="line">with open(&#39;gushi.html&#39;,&#39;w&#39;,encoding&#x3D;&#39;utf-8&#39;) as f:</span><br><span class="line">    f.write(html)</span><br><span class="line">    # 将源代码写入html中</span><br></pre></td></tr></table></figure>

<h2 id="11-雪球网站"><a href="#11-雪球网站" class="headerlink" title="11.雪球网站"></a>11.雪球网站</h2><p>Python</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">from lxml import etree</span><br><span class="line">from selenium import webdriver</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">def jiexi(res):</span><br><span class="line">    tree &#x3D; etree.HTML(res)</span><br><span class="line">    res_list &#x3D; tree.xpath(&#39;&#x2F;&#x2F;div[@class&#x3D;&quot;AnonymousHome_home__timeline_VTo&quot;]&#x2F;div&#x2F;&#x2F;div[@class&#x3D;&quot;AnonymousHome_home__timeline__item_3vU&quot;]&#39;)</span><br><span class="line">    cc&#x3D; []</span><br><span class="line">    for i in res_list:</span><br><span class="line">        h3 &#x3D; i.xpath(&#39;.&#x2F;h3&#x2F;a&#x2F;text()&#39;)</span><br><span class="line">        zuozhe &#x3D; i.xpath(&#39;.&#x2F;&#x2F;div[@class&#x3D;&quot;AnonymousHome_auchor_1RR&quot;]&#x2F;a[@class&#x3D;&quot;AnonymousHome_user-name_3wN&quot;]&#x2F;text()&#39;)</span><br><span class="line">        fabu_time &#x3D; i.xpath(&#39;.&#x2F;&#x2F;span[2]&#x2F;text()&#39;)</span><br><span class="line">        yuedu &#x3D; i.xpath(&#39;.&#x2F;&#x2F;div[@class&#x3D;&quot;AnonymousHome_read_2t5&quot;]&#x2F;text()&#39;)</span><br><span class="line">        cc.append(&#123;</span><br><span class="line">            &#39;h3&#39; : h3 ,</span><br><span class="line">            &#39;zuozhe&#39; : zuozhe ,</span><br><span class="line">            &#39;fabutime&#39; : fabu_time ,</span><br><span class="line">            &#39;yuedu&#39; : yuedu</span><br><span class="line">        &#125;)</span><br><span class="line">    return cc</span><br><span class="line"></span><br><span class="line">import csv</span><br><span class="line">def xiazai(res_list):</span><br><span class="line">    with open(&#39;xue_qiu.csv&#39;,&#39;w&#39;,encoding&#x3D;&#39;utf-8&#39;) as f:</span><br><span class="line">        writer &#x3D; csv.writer(f,delimiter&#x3D;&#39;,&#39;)</span><br><span class="line">        writer.writerow([&#39;标题&#39;,&#39;作者&#39;,&#39;发布时间&#39;,&#39;点击量&#39;])</span><br><span class="line"></span><br><span class="line">    for i in res_list:</span><br><span class="line">        try:</span><br><span class="line">            username &#x3D; i[&#39;h3&#39;][0]</span><br><span class="line">            zuozhe &#x3D; i[&#39;zuozhe&#39;][0]</span><br><span class="line">            fabutime &#x3D; i[&#39;fabutime&#39;][0]</span><br><span class="line">            yuedu &#x3D; i[&#39;yuedu&#39;][0]</span><br><span class="line">            with open(&#39;xue_qiu.csv&#39;, &#39;a&#39;, encoding&#x3D;&#39;utf-8&#39;) as f:</span><br><span class="line">                writer &#x3D; csv.writer(f, delimiter&#x3D;&#39;,&#39;)</span><br><span class="line">                writer.writerow([str(username),zuozhe, fabutime, yuedu])</span><br><span class="line">        except:</span><br><span class="line">            zuozhe &#x3D; i[&#39;zuozhe&#39;][0]</span><br><span class="line">            fabutime &#x3D; i[&#39;fabutime&#39;][0]</span><br><span class="line">            yuedu &#x3D; i[&#39;yuedu&#39;][0]</span><br><span class="line">            with open(&#39;xue_qiu.csv&#39;, &#39;a&#39;, encoding&#x3D;&#39;utf-8&#39;) as f:</span><br><span class="line">                writer &#x3D; csv.writer(f, delimiter&#x3D;&#39;,&#39;)</span><br><span class="line">                writer.writerow([&#39;无标题&#39;,zuozhe, fabutime, yuedu])</span><br><span class="line"></span><br><span class="line">def main(res):</span><br><span class="line">    url &#x3D; &#39;https:&#x2F;&#x2F;xueqiu.com&#x2F;&#39;</span><br><span class="line">    headers &#x3D; &#123;</span><br><span class="line">        &#39;User-Agent&#39; : &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;79.0.3945.88 Safari&#x2F;537.36&#39;</span><br><span class="line">    &#125;</span><br><span class="line">    res_list &#x3D; jiexi(res)</span><br><span class="line">    xiazai(res_list)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    brower &#x3D; webdriver.Chrome()</span><br><span class="line">    brower.get(&#39;https:&#x2F;&#x2F;xueqiu.com&#x2F;&#39;)</span><br><span class="line">    js &#x3D; &#39;window.scrollTo(0,document.body.scrollHeight)&#39;  # 将页面滚动到最下面</span><br><span class="line">    for i in range(1, 6):</span><br><span class="line">        if i &gt;&#x3D; 5:</span><br><span class="line">            brower.find_element_by_link_text(&#39;加载更多&#39;).click()</span><br><span class="line">            print(&#39;点击了&#39;)</span><br><span class="line">        else:</span><br><span class="line">            brower.execute_script(js) .</span><br><span class="line">            time.sleep(1)</span><br><span class="line">    res &#x3D; brower.page_source</span><br><span class="line"></span><br><span class="line">    main(res)</span><br></pre></td></tr></table></figure>

<h2 id="12-模拟QQ空间登陆-没有涉及图片滑动解锁"><a href="#12-模拟QQ空间登陆-没有涉及图片滑动解锁" class="headerlink" title="12.模拟QQ空间登陆(没有涉及图片滑动解锁)"></a>12.模拟QQ空间登陆(没有涉及图片滑动解锁)</h2><p>Python</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">from selenium import webdriver  # 操作网页的包</span><br><span class="line"></span><br><span class="line">from time import sleep  # 时间模块</span><br><span class="line"></span><br><span class="line">brower &#x3D; webdriver.Chrome()  # 实例化</span><br><span class="line">brower.get(&#39;https:&#x2F;&#x2F;qzone.qq.com&#x2F;&#39;)  # 确认要操作的网页</span><br><span class="line">brower.switch_to.frame(&#39;login_frame&#39;)  # 进入页面的子页面，QQ空间</span><br><span class="line">brower.find_element_by_id(&#39;switcher_plogin&#39;).click()</span><br><span class="line"># brower.find_element_by_id(&#39;img_out_1559878380&#39;).click()  # 快捷登陆，前提是需要登陆QQ</span><br><span class="line">sleep(4)</span><br><span class="line">user &#x3D; brower.find_element_by_id(&#39;u&#39;)  # 账户输入框的ID值</span><br><span class="line">sleep(5)</span><br><span class="line">user.clear()  # 清空账号框的内容，避免session存储了数据</span><br><span class="line">user.send_keys(&#39;1559878380&#39;)  # 在输入框内输入指定的数据</span><br><span class="line">pwd &#x3D; brower.find_element_by_id(&#39;p&#39;)  # 定位密码框的ID值</span><br><span class="line">sleep(5)</span><br><span class="line">pwd.clear()  # 清空密码框的内容</span><br><span class="line">pwd.send_keys(&#39;224949826..&#39;)  # 输入密码框的内容</span><br><span class="line">sleep(5)</span><br><span class="line">brower.find_element_by_id(&#39;login_button&#39;).click()  # 定位登陆按钮的ID值，并且点击</span><br></pre></td></tr></table></figure>

<h2 id="13-百度图片爬取-利用selenium-辉夜大小姐）"><a href="#13-百度图片爬取-利用selenium-辉夜大小姐）" class="headerlink" title="13.百度图片爬取,利用selenium (辉夜大小姐）"></a>13.百度图片爬取,利用selenium (辉夜大小姐）</h2><p>Python</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">import requests  # 导包</span><br><span class="line">from selenium import webdriver  # 操作网页的包</span><br><span class="line">from lxml import etree  # 解析包</span><br><span class="line">import time  # 导入时间模块</span><br><span class="line">from time import sleep</span><br><span class="line">brower &#x3D; webdriver.Chrome()  # 实例化</span><br><span class="line">brower.get(&#39;http:&#x2F;&#x2F;image.baidu.com&#x2F;&#39;)  # 需要操作的操作</span><br><span class="line"></span><br><span class="line">brower.find_element_by_id(&#39;kw&#39;).send_keys(&#39;辉夜大小姐&#39;)  # 获取搜索框的id值，利用send_keys输入指定值</span><br><span class="line">ret &#x3D; brower.find_element_by_class_name(&#39;s_search&#39;)  # 利用方法找到搜索按钮，</span><br><span class="line">ret.click()  # 点击搜索按钮</span><br><span class="line">sleep(2)  # 休息2s</span><br><span class="line">for i in range(3):  # 循环3次</span><br><span class="line">    js &#x3D; &#39;window.scrollTo(0,document.body.scrollHeight)&#39;  # 页面向下拉到最底部</span><br><span class="line">    brower.execute_script(js)  # 执行js代码</span><br><span class="line">    time.sleep(4)  # 休眠4s</span><br><span class="line">res &#x3D; brower.page_source  # 将页面的HTML页面的源代码 抓取</span><br><span class="line"></span><br><span class="line">terr &#x3D; etree.HTML(res)  # 实例化xpath</span><br><span class="line">ret_list &#x3D; terr.xpath(&#39;&#x2F;&#x2F;div[@id&#x3D;&quot;imgid&quot;]&#x2F;div&#x2F;&#x2F;ul&#x2F;li&#39;)  # 匹配要抓取的数据</span><br><span class="line">headers &#x3D; &#123;</span><br><span class="line">    &#39;User-Agent&#39; : &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;79.0.3945.88 Safari&#x2F;537.36&#39;</span><br><span class="line">&#125;</span><br><span class="line">for i in ret_list:</span><br><span class="line">    url_li &#x3D; i.xpath(&#39;.&#x2F;@data-objurl&#39;)[0]</span><br><span class="line">    res &#x3D; requests.get(url&#x3D;url_li,headers&#x3D;headers).content</span><br><span class="line">    name &#x3D; url_li.split(&#39;&#x2F;&#39;)[-1]</span><br><span class="line">    with open(&#39;.&#x2F;imgs&#x2F;%s&#39;%name,&#39;wb&#39;) as f:</span><br><span class="line">        f.write(res)</span><br></pre></td></tr></table></figure>

<h2 id="14-谷歌无头浏览器"><a href="#14-谷歌无头浏览器" class="headerlink" title="14.谷歌无头浏览器"></a>14.谷歌无头浏览器</h2><p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20191225083740.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20191225083740.png" alt="微信截图_20191225083740"></a></p>
<h1 id="三-多线程爬虫"><a href="#三-多线程爬虫" class="headerlink" title="三.多线程爬虫"></a>三.多线程爬虫</h1><h2 id="1-并发和并行"><a href="#1-并发和并行" class="headerlink" title="1.并发和并行"></a>1.并发和并行</h2><ul>
<li>并发:同一时间段内共同执行<ul>
<li>时间片轮转法:给每一个程序一个时间段，一点一点执行，可能导致结果出错(不加锁的情况下)</li>
<li>多线程可能会导致数据不安全(上锁可以避免此情况)</li>
</ul>
</li>
<li>并行：同一时刻一起执行</li>
</ul>
<h2 id="2-爬虫架构图"><a href="#2-爬虫架构图" class="headerlink" title="2.爬虫架构图"></a>2.爬虫架构图</h2><ol>
<li>URL队列</li>
<li>爬虫线程类</li>
<li>data队列</li>
<li>数据解析的线程类</li>
</ol>
<h2 id="3-多线程爬虫实例-存储的是csv格式"><a href="#3-多线程爬虫实例-存储的是csv格式" class="headerlink" title="3.多线程爬虫实例(存储的是csv格式)"></a>3.多线程爬虫实例(存储的是csv格式)</h2><p>Python</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line">import threading</span><br><span class="line">import requests</span><br><span class="line">from lxml import etree</span><br><span class="line">from queue import Queue</span><br><span class="line">from threading import Lock</span><br><span class="line">import csv</span><br><span class="line">#封装爬虫类(负责数据采集的)</span><br><span class="line">class CrawlThread(threading.Thread):</span><br><span class="line">    def __init__(self,crawlName,pageQueue,dataQueue):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.crawlname &#x3D; crawlName  # 爬虫名，用于监控爬虫的运行过程</span><br><span class="line">        self.pageQueue &#x3D; pageQueue  # url的队列，用于存放url</span><br><span class="line">        self.dataQueue &#x3D; dataQueue  # 存放响应数据</span><br><span class="line">        self.headers &#x3D; &#123;</span><br><span class="line">            &#39;User-Agent&#39; : &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;79.0.3945.88 Safari&#x2F;537.36&#39;</span><br><span class="line">        &#125;  # UA伪装</span><br><span class="line">    def run(self):</span><br><span class="line">        base_url &#x3D; &#39;http:&#x2F;&#x2F;xiaohua.zol.com.cn&#x2F;lengxiaohua&#x2F;%s.html&#39;  # 定义一个基准的url</span><br><span class="line">        while True:</span><br><span class="line">            try:</span><br><span class="line">                page &#x3D; self.pageQueue.get(block&#x3D;False)</span><br><span class="line">                # block参数代表队列参数是否阻塞</span><br><span class="line">                # 如果block &#x3D; Ture 代表阻塞，如果队列为空，在get时，会产生阻塞，知道队列有元素可以get 时为止，可是获取元素</span><br><span class="line">                # block &#x3D; False 代表不阻塞，如果队列为空，get的操作会抛出异常</span><br><span class="line">                print(&#39;%s正在爬取数据&#39;%self.crawlname)</span><br><span class="line">                # 请求页面获取响应数据</span><br><span class="line">                res &#x3D; requests.get(url&#x3D;base_url%page,headers&#x3D;self.headers).text</span><br><span class="line">                # 将响应数据提交给dataQueue，以便后续的解析的线程取用</span><br><span class="line">                self.dataQueue.put(res)</span><br><span class="line">                print(&#39;%s提交数据完毕&#39;%self.crawlname)</span><br><span class="line">            except:</span><br><span class="line">                break</span><br><span class="line"></span><br><span class="line">#解析类(负责数据的解析和存储)</span><br><span class="line">class PaarseThead(threading.Thread):</span><br><span class="line">    def __init__(self,p,dataQueue,lock):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.PaarseName &#x3D; p</span><br><span class="line">        self.dataQueue &#x3D; dataQueue</span><br><span class="line">        self.lock &#x3D; lock</span><br><span class="line">    def run(self):</span><br><span class="line">        while True:</span><br><span class="line">            try:</span><br><span class="line">                html &#x3D; self.dataQueue.get(block&#x3D;False)</span><br><span class="line">                print(&#39;%s正在解析数据&#39;%self.PaarseName)</span><br><span class="line">                self.parse(html)</span><br><span class="line">                print(&#39;%s数据数据解析完毕&#39;%self.PaarseName)</span><br><span class="line">            except:</span><br><span class="line">                break</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def parse(self,html):</span><br><span class="line">        # 具体的解析过程</span><br><span class="line">        tree &#x3D; etree.HTML(html)</span><br><span class="line">        res &#x3D; tree.xpath(&#39;&#x2F;&#x2F;li[@class&#x3D;&quot;article-summary&quot;]&#39;)</span><br><span class="line">        self.csv_t()</span><br><span class="line">        for i in res:</span><br><span class="line">            title &#x3D; i.xpath(&#39;.&#x2F;span[2]&#x2F;a&#x2F;text()&#39;)[0]</span><br><span class="line">            href &#x3D; i.xpath(&#39;.&#x2F;span[2]&#x2F;a&#x2F;@href&#39;)[0]</span><br><span class="line">            content &#x3D; i.xpath(&#39;.&#x2F;&#x2F;div[@class&#x3D;&quot;summary-text&quot;]&#x2F;text()&#39;)</span><br><span class="line">            d &#x3D; &#123;&#39;title&#39; : title,&#39;href&#39;:href,&#39;content&#39;:content&#125;</span><br><span class="line">            with self.lock:</span><br><span class="line">                self.save(d)</span><br><span class="line"></span><br><span class="line">    def csv_t(self):</span><br><span class="line">        with open(&#39;data.csv&#39;,&#39;w&#39;,encoding&#x3D;&#39;utf-8&#39;) as f:</span><br><span class="line">            writer &#x3D; csv.writer(f,delimiter&#x3D;&#39;,&#39;)</span><br><span class="line">            writer.writerow([&#39;标题&#39;,&#39;链接&#39;,&#39;内容&#39;])</span><br><span class="line">    def save(self,d):</span><br><span class="line">        with open(&#39;data.csv&#39;,&#39;a&#39;,encoding&#x3D;&#39;utf-8&#39;) as f:</span><br><span class="line">            writer &#x3D; csv.writer(f,delimiter&#x3D;&#39;,&#39;)</span><br><span class="line">            writer.writerow([d[&#39;title&#39;],d[&#39;href&#39;],d[&#39;content&#39;][0]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    pageQueue &#x3D; Queue()  # 存放url的队列</span><br><span class="line">    dataQueue &#x3D; Queue()  # 存放响应数据的队列</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    发起请求和获取响应</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    for i in range(1,20):</span><br><span class="line">        pageQueue.put(i)  # 向url队列添加列表</span><br><span class="line"></span><br><span class="line">    CrawlName &#x3D; [&#39;爬虫一号&#39;,&#39;爬虫二号&#39;,&#39;爬虫三号&#39;]</span><br><span class="line">    crawlList &#x3D; []</span><br><span class="line"></span><br><span class="line">    for var in CrawlName:  #实例化爬虫线程对象</span><br><span class="line">        c &#x3D; CrawlThread(var,pageQueue,dataQueue)</span><br><span class="line">        c.start()  # 开启线程</span><br><span class="line">        crawlList.append(c)</span><br><span class="line"></span><br><span class="line">    for c in crawlList:  # 将线程统一执行join操作</span><br><span class="line">        c.join()</span><br><span class="line"></span><br><span class="line">    print(dataQueue.qsize())  # 查看dataQueue队列中有几条数据</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    解析数据</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    lock &#x3D; Lock()</span><br><span class="line">    PaarseName &#x3D; [&#39;解析1号&#39;,&#39;解析2号&#39;,&#39;解析3号&#39;]</span><br><span class="line">    PaarseList &#x3D; []</span><br><span class="line">    for p in PaarseName:</span><br><span class="line">        cc &#x3D; PaarseThead(p,dataQueue,lock)</span><br><span class="line">        cc.start()</span><br><span class="line">        PaarseList.append(cc)</span><br><span class="line">    for pl in PaarseList:</span><br><span class="line">        pl.join()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>

<h2 id="4-多线程爬取辽宁工程"><a href="#4-多线程爬取辽宁工程" class="headerlink" title="4.多线程爬取辽宁工程"></a>4.多线程爬取辽宁工程</h2><p>Python</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line">from lxml import etree  # 导包lxml包</span><br><span class="line">import requests  </span><br><span class="line">import pymongo</span><br><span class="line">from queue import Queue</span><br><span class="line">import threading</span><br><span class="line">from threading import Lock</span><br><span class="line"></span><br><span class="line">conn &#x3D; pymongo.MongoClient(&#39;127.0.0.1&#39;,27017)  # 实例化mangodb数据库</span><br><span class="line">db &#x3D; conn.ztb  # 要使用的数据库，如果该数据库则创建新的数据库</span><br><span class="line">table &#x3D; db.lngc  # 使用该表，如果没有该表，则创建该表</span><br><span class="line"></span><br><span class="line">class CrawlThread(threading.Thread):  # 爬虫类</span><br><span class="line">    def __init__(self,crawname,urlqueue,dataqueue,headers):</span><br><span class="line">        super().__init__()  # 使init不报黄</span><br><span class="line">        self.crawname &#x3D; crawname  # 将当前爬虫(爬虫1号or爬虫2号等) 传值过来，方便显示</span><br><span class="line">        self.urlqueue &#x3D; urlqueue  # 将url队列传过来</span><br><span class="line">        self.dataqueue &#x3D; dataqueue  # 响应值队列</span><br><span class="line">        self.headers &#x3D; headers  # 请求头</span><br><span class="line"></span><br><span class="line">    def run(self):</span><br><span class="line">       while True:  # 无限循环</span><br><span class="line">           try:</span><br><span class="line">               print(&#39;%s正在抓取数据页面&#39;%self.crawname)  # 在后端显示当前是几号爬虫在爬取数据</span><br><span class="line">               res &#x3D; requests.get(url&#x3D;self.urlqueue.get(block&#x3D;False),headers&#x3D;self.headers).text  # 将url队列中的路由取出来，队列的get操作每次只获取一个数据</span><br><span class="line">               print(&#39;%s正在抓取完毕&#39;%self.crawname)</span><br><span class="line">               self.dataqueue.put(res)  # 将获取到响应数据 添加到响应数据队列</span><br><span class="line">           except:</span><br><span class="line">               break  # 因为 get()有 block&#x3D;False 如果取值下一个没有数据 则报错 报错就会终止该循环</span><br><span class="line"></span><br><span class="line">class ParseThead(threading.Thread):  # 解析类</span><br><span class="line">    def __init__(self,parse,dataqueue,lock):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.parse &#x3D; parse  # 将当前解析对象赋值给self.parse</span><br><span class="line">        self.dataqueue &#x3D; dataqueue  # 将响应数据队列传过来，注意 此时的队列已经是有数据</span><br><span class="line">        self.lock &#x3D; lock  # 锁</span><br><span class="line"></span><br><span class="line">    def run(self):</span><br><span class="line">        while True:</span><br><span class="line">            try:</span><br><span class="line">                print(&#39;%s正在解析数据&#39;%self.parse)</span><br><span class="line">                ret &#x3D; self.dataqueue.get(block&#x3D;False)  # 将队列中的数据取出来，一次只能取一个</span><br><span class="line">                print(&#39;%s解析数据完毕&#39;%self.parse)</span><br><span class="line">                self.parsejiexi(ret)  # 将获取的值作为实参传给函数parsejiexi</span><br><span class="line">            except:</span><br><span class="line">                break</span><br><span class="line">    def parsejiexi(self,cc):</span><br><span class="line">        tree &#x3D; etree.HTML(cc)  # 实例化etree</span><br><span class="line">        ret &#x3D; tree.xpath(&#39;&#x2F;&#x2F;ul[@id&#x3D;&quot;showList&quot;]&#x2F;li&#39;)  # 解析页面，获取当前页所有的li</span><br><span class="line">        for i in ret:  # 循环所有的li</span><br><span class="line">            title &#x3D; i.xpath(&#39;.&#x2F;&#x2F;a&#x2F;font[1]&#x2F;text()&#39;)[0]  # 获取每个li中的要获取的值</span><br><span class="line">            href &#x3D; i.xpath(&#39;.&#x2F;&#x2F;a&#x2F;@href&#39;)[0]</span><br><span class="line">            laizi &#x3D; i.xpath(&#39;.&#x2F;&#x2F;a&#x2F;font[2]&#x2F;text()&#39;)[0]</span><br><span class="line">            biaoti &#x3D; i.xpath(&#39;.&#x2F;&#x2F;a&#x2F;text()&#39;)[0]</span><br><span class="line">            time &#x3D; i.xpath(&#39;.&#x2F;span&#x2F;text()&#39;)[0]</span><br><span class="line">            d &#x3D; &#123;&#39;title&#39;:title,&#39;href&#39;:&#39;http:&#x2F;&#x2F;lnzxzb.cn&#39;+href,&#39;laizi&#39;:laizi,&#39;biaoti&#39;:biaoti,&#39;time&#39;:time&#125;  # 将获取的值 存放在字典中，mongodb存储格式也是字典 所以直接可以添加到mogodb中</span><br><span class="line">               with self.lock:  #开启锁，如果当前代码结束就解锁</span><br><span class="line">            		self.mongodb_insert(d)  # 调用数据持久化的函数</span><br><span class="line">    def mongodb_insert(self,d):  # 数据持久化的函数</span><br><span class="line">        table.insert_one(d)  # 将数据存储在数据库中</span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    urlqueue &#x3D; Queue()  # 实例化url队列</span><br><span class="line">    dataqueue &#x3D; Queue()  # 实例化响应队列</span><br><span class="line">    shouye_url &#x3D; [&#39;http:&#x2F;&#x2F;lnzxzb.cn&#x2F;gcjyxx&#x2F;subpage.html&#39;]  # 首页路由</span><br><span class="line">    yema &#x3D; [&#39;http:&#x2F;&#x2F;lnzxzb.cn&#x2F;gcjyxx&#x2F;%s.html&#39;%page for page in range(1,51)]  # 列表推导式 推导出50个页面</span><br><span class="line">    shouye_url.extend(yema)  # 拆分添加</span><br><span class="line">    for i in shouye_url:  # 将所有页面循环</span><br><span class="line">        urlqueue.put(i)  # 将url添加到url队列中</span><br><span class="line"></span><br><span class="line">    headers &#x3D; &#123;</span><br><span class="line">        &#39;user-Agent&#39; : &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;79.0.3945.88 Safari&#x2F;537.36&#39;</span><br><span class="line">    &#125;  # 表头</span><br><span class="line"></span><br><span class="line">    CrawlName &#x3D; [&#39;爬虫1号&#39;,&#39;爬虫2号&#39;,&#39;爬虫3号&#39;,&#39;爬虫4号&#39;,&#39;爬虫5号&#39;,&#39;爬虫6号&#39;,&#39;爬虫7号&#39;]  # 随便写几个爬虫</span><br><span class="line">    CrawList &#x3D; []  # 做一个空列表</span><br><span class="line">    for craw in CrawlName:  # 循环列表 因为CrawName 有7个 所以开了7个线程</span><br><span class="line">        c &#x3D; CrawlThread(craw,urlqueue,dataqueue,headers)  # 实例化线程</span><br><span class="line">        c.start()  # 开启线程</span><br><span class="line">        CrawList.append(c)  # 将线程的变量存储到空列表中</span><br><span class="line">    for c in CrawList:</span><br><span class="line">        c.join()  # 等待线程结束才关闭主线程</span><br><span class="line"></span><br><span class="line">    # 下面的这些和上面一个意思  就是 开启的线程是 4 个线程      </span><br><span class="line">    ParseName &#x3D; [&#39;解析1号&#39;,&#39;解析2号&#39;,&#39;解析3号&#39;,&#39;解析4号&#39;]</span><br><span class="line">    ParseList &#x3D; []</span><br><span class="line">    ParList &#x3D; []</span><br><span class="line">    lock &#x3D; Lock()</span><br><span class="line">    for parse in ParseName:</span><br><span class="line">        p &#x3D; ParseThead(parse,dataqueue,lock)</span><br><span class="line">        p.start()</span><br><span class="line">        ParseList.append(p)</span><br><span class="line">    for p in ParList:</span><br><span class="line">        p.join()</span><br><span class="line">    # print(dataqueue.qsize())</span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>

<h1 id="四-第三方打码平台-超级鹰"><a href="#四-第三方打码平台-超级鹰" class="headerlink" title="四.第三方打码平台(超级鹰)"></a>四.第三方打码平台(超级鹰)</h1><h2 id="1-打码平台代码"><a href="#1-打码平台代码" class="headerlink" title="1.打码平台代码"></a>1.打码平台代码</h2><p>Python</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F;env python</span><br><span class="line"># coding:utf-8</span><br><span class="line"></span><br><span class="line">import requests</span><br><span class="line">from hashlib import md5</span><br><span class="line"></span><br><span class="line">class Chaojiying_Client(object):</span><br><span class="line"></span><br><span class="line">    def __init__(self, username, password, soft_id):</span><br><span class="line">            self.username &#x3D; username</span><br><span class="line">            password &#x3D;  password.encode(&#39;utf8&#39;)</span><br><span class="line">            self.password &#x3D; md5(password).hexdigest()</span><br><span class="line">            self.soft_id &#x3D; soft_id</span><br><span class="line">            self.base_params &#x3D; &#123;</span><br><span class="line">                &#39;user&#39;: self.username,</span><br><span class="line">                &#39;pass2&#39;: self.password,</span><br><span class="line">                &#39;softid&#39;: self.soft_id,</span><br><span class="line">            &#125;</span><br><span class="line">            self.headers &#x3D; &#123;</span><br><span class="line">                &#39;Connection&#39;: &#39;Keep-Alive&#39;,</span><br><span class="line">                &#39;User-Agent&#39;: &#39;Mozilla&#x2F;4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident&#x2F;4.0)&#39;,</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">    def PostPic(self, im, codetype):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        im: 图片字节</span><br><span class="line">        codetype: 题目类型 参考 http:&#x2F;&#x2F;www.chaojiying.com&#x2F;price.html</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        params &#x3D; &#123;</span><br><span class="line">            &#39;codetype&#39;: codetype,</span><br><span class="line">        &#125;</span><br><span class="line">        params.update(self.base_params)</span><br><span class="line">        files &#x3D; &#123;&#39;userfile&#39;: (&#39;ccc.jpg&#39;, im)&#125;</span><br><span class="line">        r &#x3D; requests.post(&#39;http:&#x2F;&#x2F;upload.chaojiying.net&#x2F;Upload&#x2F;Processing.php&#39;, data&#x3D;params, files&#x3D;files, headers&#x3D;self.headers)</span><br><span class="line">        return r.json()</span><br><span class="line"></span><br><span class="line">    def ReportError(self, im_id):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        im_id:报错题目的图片ID</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        params &#x3D; &#123;</span><br><span class="line">            &#39;id&#39;: im_id,</span><br><span class="line">        &#125;</span><br><span class="line">        params.update(self.base_params)</span><br><span class="line">        r &#x3D; requests.post(&#39;http:&#x2F;&#x2F;upload.chaojiying.net&#x2F;Upload&#x2F;ReportError.php&#39;, data&#x3D;params, headers&#x3D;self.headers)</span><br><span class="line">        return r.json()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">        chaojiying &#x3D; Chaojiying_Client(&#39;1559878380&#39;, &#39;224949826..&#39;, &#39;902931&#39;)	#用户中心&gt;&gt;软件ID 生成一个替换 96001</span><br><span class="line">        im &#x3D; open(&#39;a.jpg&#39;, &#39;rb&#39;).read()													#本地图片文件路径 来替换 a.jpg 有时WIN系统须要&#x2F;&#x2F;</span><br><span class="line">        print (chaojiying.PostPic(im, 1902))												#1902 验证码类型  官方网站&gt;&gt;价格体系 3.4+版 print 后要加()</span><br></pre></td></tr></table></figure>

<h1 id="五-反爬"><a href="#五-反爬" class="headerlink" title="五.反爬"></a>五.反爬</h1><h2 id="1-selenium自动化操作中检测到security-check问题-cookie时效性差的问题"><a href="#1-selenium自动化操作中检测到security-check问题-cookie时效性差的问题" class="headerlink" title="1. selenium自动化操作中检测到security-check问题(cookie时效性差的问题)"></a>1. selenium自动化操作中检测到security-check问题(cookie时效性差的问题)</h2><p>Python</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from selenium.webdriver import ChromeOptions  # 可以避免浏览器检测的包</span><br><span class="line">options &#x3D; webdriver.ChromeOptions()  # 实例化</span><br><span class="line">options.add_experimental_option(&#39;excludeSwitches&#39;, [&#39;enable-automation&#39;])  # 给实例化的属性添加一个键值对，这个键值对就可以避免网站检测到是否是爬虫</span><br><span class="line">browser &#x3D; webdriver.Chrome(options&#x3D;options)  # 将该属性传进来</span><br><span class="line"># 最新版(79版)不能使用需要降版本到78版本</span><br></pre></td></tr></table></figure>

<h1 id="六-Scrapy框架"><a href="#六-Scrapy框架" class="headerlink" title="六.Scrapy框架"></a>六.Scrapy框架</h1><h2 id="1-安装-1"><a href="#1-安装-1" class="headerlink" title="1.安装"></a>1.安装</h2><p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191231103007.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191231103007.png" alt="QQ截图20191231103007"></a></p>
<p>Python</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http:&#x2F;&#x2F;www.lfd.uci.edu&#x2F;~gohlke&#x2F;pythonlibs&#x2F;  # twisted的安装网页,目前已经安装好了</span><br></pre></td></tr></table></figure>

<h2 id="2-五大核心组件与数据流向"><a href="#2-五大核心组件与数据流向" class="headerlink" title="2.五大核心组件与数据流向"></a>2.五大核心组件与数据流向</h2><p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191231103504.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20191231103504.png" alt="QQ截图20191231103504"></a></p>
<ul>
<li>Scrapy Engine 引擎 负责组件间通信</li>
<li>Item Pipeline 管道 负责数据持久化</li>
<li>Downloader 下载器 负责爬取数据</li>
<li>Scheduler 调度器 负责调度请求</li>
<li>Spider 爬虫 定义了爬取行为，数据解析规则，提交item给管道</li>
<li>Downloader Middlewares 下载中间间</li>
<li>Spider Middlewares 爬虫中间件</li>
</ul>
<p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/IMG_20191231_104737_1.jpg"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/IMG_20191231_104737_1.jpg" alt="IMG_20191231_104737_1"></a></p>
<h2 id="3-创建一个项目"><a href="#3-创建一个项目" class="headerlink" title="3.创建一个项目"></a>3.创建一个项目</h2><p>Python</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject ztb  # 创建scrapy项目名为ztb</span><br><span class="line">scrapy genspider tb baidu.com  # 定义爬虫名字</span><br><span class="line">scrapy crawl tb --nolog  # 启动爬虫项目， --nolog为不显示日志</span><br><span class="line">typ &#x3D; r.xpath(&#39;.&#x2F;p&#x2F;a&#x2F;font[1]&#x2F;text()&#39;).extract_first()  # xpath解析数据为一个对象需要使用extract_first()取出第一个值</span><br></pre></td></tr></table></figure>

<h2 id="4-获取详情页"><a href="#4-获取详情页" class="headerlink" title="4.获取详情页"></a>4.获取详情页</h2><p>Python</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">class XhSpider(scrapy.Spider):</span><br><span class="line">    name &#x3D; &#39;xh&#39;</span><br><span class="line">    # allowed_domains &#x3D; [&#39;baidu.com&#39;]</span><br><span class="line">    start_urls &#x3D; [&#39;http:&#x2F;&#x2F;www.jokeji.cn&#x2F;list_%s.htm&#39;%page for page in range(1,3)]</span><br><span class="line"></span><br><span class="line">    def com_parse(self,response):  # 接收到详情页的响应</span><br><span class="line">        item &#x3D; response.meta[&#39;item&#39;]  # 将函数parse中的meta中的item取出来赋值给item</span><br><span class="line">        content &#x3D; &#39;&#39;.join(response.xpath(&#39;&#x2F;&#x2F;span[@id&#x3D;&quot;text110&quot;]&#x2F;&#x2F;text()&#39;).extract())  # 将所有文本写入 并且利用字符串的常见操作join拼接到新的字符串中</span><br><span class="line">        item[&#39;content&#39;] &#x3D; content  # 将这个字符串储存在itmen的对应的key中</span><br><span class="line">        yield item  # 返回item值</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        li_list &#x3D; response.xpath(&#39;&#x2F;&#x2F;div[@class&#x3D;&quot;list_title&quot;]&#x2F;ul&#x2F;li&#39;)</span><br><span class="line">        for li in li_list:</span><br><span class="line">            title &#x3D; li.xpath(&#39;.&#x2F;b&#x2F;a&#x2F;text()&#39;).extract_first()</span><br><span class="line">            times &#x3D; li.xpath(&#39;.&#x2F;i&#x2F;text()&#39;).extract_first()</span><br><span class="line">            liulan &#x3D; li.xpath(&#39;.&#x2F;span&#x2F;text()&#39;).extract_first()</span><br><span class="line">            url_c &#x3D; &#39;http:&#x2F;&#x2F;www.jokeji.cn&#39; + li.xpath(&#39;.&#x2F;b&#x2F;a&#x2F;@href&#39;).extract_first()</span><br><span class="line">            item &#x3D; XiaohuaItem()</span><br><span class="line">            item[&#39;title&#39;] &#x3D; title</span><br><span class="line">            item[&#39;times&#39;] &#x3D; times</span><br><span class="line">            item[&#39;liulan&#39;] &#x3D; liulan</span><br><span class="line">            yield scrapy.Request(url&#x3D;url_c,callback&#x3D;self.com_parse,meta&#x3D;&#123;&#39;item&#39;:item&#125;)  # 用新的url发送请求，并且利用callback定位到新的函数中，并且把meta的值 以字典的形式传过去</span><br></pre></td></tr></table></figure>

<h2 id="5-动态获取数据scrapy框架和selenium框架结合使用-网易新闻"><a href="#5-动态获取数据scrapy框架和selenium框架结合使用-网易新闻" class="headerlink" title="5.动态获取数据scrapy框架和selenium框架结合使用(网易新闻)"></a>5.动态获取数据scrapy框架和selenium框架结合使用(网易新闻)</h2><p>Python</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def process_response(self, request, response, spider):  # 下载中间件的拦截全部响应</span><br><span class="line">    if response.url in spider.start_urls:  # 判断当前路由是否是静态页面的url</span><br><span class="line">        bowor &#x3D; spider.bower  # 从spider中导出实例化的selenium对象</span><br><span class="line">        bowor.get(response.url)  # selenium访问url</span><br><span class="line">        js &#x3D; &#39;window.scrollTo(0, document.body.scrollHeight)&#39;</span><br><span class="line">        for i in range(2):</span><br><span class="line">            bowor.execute_script(js)</span><br><span class="line">            time.sleep(2)</span><br><span class="line">        html &#x3D; bowor.page_source  # 获取网页源码</span><br><span class="line">        return HtmlResponse(url&#x3D;bowor.current_url,body&#x3D;html,request&#x3D;request, encoding&#x3D;&#39;utf-8&#39;)  # 返回一个response对象 current_url为selenium当前访问的url,body为网页源码 HtmlResponse需要导包  from scrapy.http import HtmlResponse</span><br><span class="line">    return response</span><br><span class="line"># 需要打开settings的下载中间件，切记别打开错误的中间件，不然中间件不会被启用</span><br></pre></td></tr></table></figure>

<h2 id="6-数据持久化"><a href="#6-数据持久化" class="headerlink" title="6.数据持久化"></a>6.数据持久化</h2><p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20200103102800.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20200103102800.png" alt="QQ截图20200103102800"></a></p>
<p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20200103103109.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20200103103109.png" alt="QQ截图20200103103109"></a></p>
<h2 id="7-数据持久化的MongoDB的正规写法-管道"><a href="#7-数据持久化的MongoDB的正规写法-管道" class="headerlink" title="7.数据持久化的MongoDB的正规写法(管道)"></a>7.数据持久化的MongoDB的正规写法(管道)</h2><p>Python</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import pymongo  # 导包，需要mongo</span><br><span class="line">class QuanzhanPaquPipeline(object):  # 管道中自带class类</span><br><span class="line"></span><br><span class="line">    def __init__(self, mongo_uri, mongo_db):  # 初始化</span><br><span class="line">        self.mongo_uri &#x3D; mongo_uri</span><br><span class="line">        self.mongo_db &#x3D; mongo_db</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def from_crawler(cls, crawler):</span><br><span class="line">        return cls(</span><br><span class="line">            mongo_uri &#x3D; crawler.settings.get(&#39;MONGOURI&#39;),  # 在settings中配置的文件</span><br><span class="line">            mongo_db &#x3D; crawler.settings.get(&#39;MONGODB&#39;)  # uri为数据库地址，db为这次的库名</span><br><span class="line">        )</span><br><span class="line">    def open_spider(self):  # 启动爬虫时，该函数执行 连接上数据库(还有一个start_spider方法)</span><br><span class="line">        self.client &#x3D; pymongo.MongoClient(self.mongo_uri)  # mongo连接</span><br><span class="line">        self.db &#x3D; self.client[self.mongo_db]  # 创建数据库</span><br><span class="line">    def process_item(self, item, spider):  # 添加数据的时候启动</span><br><span class="line">        self.db[&#39;qz&#39;].insert_one(dict(item))  # 将数据写入到qz的表中</span><br><span class="line">        return item</span><br><span class="line">    def close_spider(self, spider):  # 爬虫结束时运行</span><br><span class="line">        self.client.close()  # 关闭数据库</span><br></pre></td></tr></table></figure>

<h2 id="9-全栈数据爬取"><a href="#9-全栈数据爬取" class="headerlink" title="9.全栈数据爬取"></a>9.全栈数据爬取</h2><p>Python</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#创建项目的时候</span><br><span class="line">scrapy startproject 项目名  # 创建项目</span><br><span class="line">scrapy genspider -t crawl 爬虫名 baidu.com  # 创建爬虫名</span><br></pre></td></tr></table></figure>

<h2 id="10-框架封装cookie"><a href="#10-框架封装cookie" class="headerlink" title="10.框架封装cookie"></a>10.框架封装cookie</h2><p>Python</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line">cookoe_str &#x3D; &#39;一个网页的cookie值&#39;</span><br><span class="line">cookies &#x3D; dict()</span><br><span class="line">from item in cookie_str.split(&#39;;&#39;)  # cookie值中每个key和value中有；分开</span><br><span class="line">	key,value &#x3D; item.split(&#39;&#x3D;&#39;,1)  # 只分割一次</span><br><span class="line">    cookies[key] &#x3D; value</span><br><span class="line">class 爬虫():</span><br><span class="line">    ......</span><br><span class="line">    def start_request(self):</span><br><span class="line">        yield scrapy.Request(url&#x3D;self.start_urls[0], cookies&#x3D;cookies)</span><br></pre></td></tr></table></figure>

<p><a href="file:///D:/%5C%E8%B5%84%E6%96%99%5C%E7%AC%AC%E5%85%AD%E4%B8%AA%E6%9C%88%5Cmd%E5%9B%BE%E7%89%87%5CQQ%E6%88%AA%E5%9B%BE20200112192742.png"><img src="file:///D:/%5C%E8%B5%84%E6%96%99%5C%E7%AC%AC%E5%85%AD%E4%B8%AA%E6%9C%88%5Cmd%E5%9B%BE%E7%89%87%5CQQ%E6%88%AA%E5%9B%BE20200112192742.png" alt="QQ截图20200112192742"></a></p>
<h1 id="七-中间件"><a href="#七-中间件" class="headerlink" title="七.中间件"></a>七.中间件</h1><p><a href="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20200102103800.png"><img src="https://baizixuan.gitee.io/my_technology_blog_hexo/markdown_img/QQ%E6%88%AA%E5%9B%BE20200102103800.png" alt="QQ截图20200102103800"></a></p>
<p>Python</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 在middleware中,中间件的函数意思</span><br><span class="line">process_request : 拦截的是非异常请求,一般可以在这里更换UA和代理IP</span><br><span class="line">process_response : 拦截的是所有响应，一般在这里可以配合selenium框架使用动态页面的抓取</span><br><span class="line">process_exception : 拦截的是异常请求</span><br></pre></td></tr></table></figure>

<h2 id="1-UA池"><a href="#1-UA池" class="headerlink" title="1.UA池"></a>1.UA池</h2><p>Python</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from fake_useragent import UserAgent  # 导包</span><br><span class="line">ua &#x3D; UserAgent()  # 实例化UserAgent方法</span><br><span class="line">ua_list &#x3D; []  # 创建一个空列表</span><br><span class="line">for i in range(200):  # 循环200次，获取200个UA</span><br><span class="line">    ua_list.append(ua.Chrome)</span><br></pre></td></tr></table></figure>

<h2 id="2-拦截之后更换IP"><a href="#2-拦截之后更换IP" class="headerlink" title="2.拦截之后更换IP"></a>2.拦截之后更换IP</h2><p>Python</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def process_request(self, request, spider):  # 在middlewares中，拦截非异常请求</span><br><span class="line">    request.meta[&#39;proxy&#39;] &#x3D; &#39;http:&#x2F;&#x2F;60.13.42.175:9999&#39;  # 前面为固定格式，后面写需要代理的ip</span><br><span class="line">    request.meta[&#39;proxy&#39;] &#x3D; &#39;http:&#x2F;&#x2F;%s&#39;%(random.choice(代理ip池))</span><br><span class="line">    return None</span><br></pre></td></tr></table></figure>

<h2 id="3-拦截之后更换UA"><a href="#3-拦截之后更换UA" class="headerlink" title="3.拦截之后更换UA"></a>3.拦截之后更换UA</h2><p>Python</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def process_request(self, request, spider):  # 在middlewares中，拦截非异常请求</span><br><span class="line">    request.headers[&#39;User-Agent&#39;] &#x3D; &#39;&#39;  # 前面为固定格式，后面写需要的UA</span><br><span class="line">    request.headers[&#39;User-Agent&#39;] &#x3D; &#39;%s&#39;%(random.choice(UA池))</span><br><span class="line">    return None</span><br><span class="line"># 然后可以在response函数中print(response.headers)打印显示</span><br></pre></td></tr></table></figure>

<h1 id="八-增量式爬虫和深度爬虫"><a href="#八-增量式爬虫和深度爬虫" class="headerlink" title="八.增量式爬虫和深度爬虫"></a>八.增量式爬虫和深度爬虫</h1><h2 id="1-增量式爬虫"><a href="#1-增量式爬虫" class="headerlink" title="1.增量式爬虫"></a>1.增量式爬虫</h2><p>Python</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 在提交请求或者保存数据的时候，判断是否存储过该url或者数据</span><br><span class="line"># 利用redis数据库的集合特点，如果没有返回1并将数据存储在数据库中</span><br><span class="line"># from redis import Redis 导包</span><br><span class="line"># 实例化一个Redis</span><br><span class="line"># ret &#x3D; conn(实例化的对象).sadd(&#39;key&#39;,&#39;value&#39;)</span><br></pre></td></tr></table></figure></div><div class="article-tags size-small is-uppercase mb-4"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted mr-2" rel="tag" href="/tags/python/">python</a><a class="link-muted mr-2" rel="tag" href="/tags/%E7%88%AC%E8%99%AB/">爬虫</a><p class="text-right font1_1"><time datetime="2020-04-22T05:52:53.356Z"><strong><em> 本文最后修改于: 2020-04-22.</em></strong></time></p></div><ul class="post-copyright"><li><strong>本文标题：</strong><a href="https://Sun-JunYu.github.io/posts/468ef90b/">关于爬虫自己所了解的功能梳理</a></li><li><strong>本文作者：</strong><a href="https://Sun-JunYu.github.io">孙军雨的技术博客</a></li><li><strong>本文链接：</strong><a href="https://Sun-JunYu.github.io/posts/468ef90b/">https://Sun-JunYu.github.io/posts/468ef90b/</a></li><li><strong>版权声明：</strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明出处！</li></ul><div class="recommend-post"><span class="is-size-6 has-text-grey has-mr-7"># 推荐文章</span><br><span>  1.<a class="is-size-6" href="/posts/a49f841a/" target="_blank">中华人民共和国刑事诉讼法</a><br></span><span>  2.<a class="is-size-6" href="/posts/7eb35566/" target="_blank">git的基本操作</a><br></span><span>  3.<a class="is-size-6" href="/posts/468ef90b/" target="_blank">关于爬虫自己所了解的功能梳理</a><br></span><span>  4.<a class="is-size-6" href="/posts/89986499/" target="_blank">python内置的内存管理机制</a><br></span><span>  5.<a class="is-size-6" href="/posts/2ff1809f/" target="_blank">python的数据结构</a><br></span></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><div class="social-share"></div><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button is-info donate"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="http://q92ekyelb.bkt.clouddn.com/zhifubao.jpg" alt="支付宝"></span></a><a class="button is-success donate"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="http://q92ekyelb.bkt.clouddn.com/weixin.png" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/posts/468ef951/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">vue基础语法</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/posts/7eb35566/"><span class="level-item">git的基本操作</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.6.0/dist/gitalk.css"><script src="https://cdn.jsdelivr.net/gh/removeif/removeif.github.io@v2.2.1/js/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: 'd1d93ece00a6aa09046fd549769ddff2',
            repo: 'Sun-JunYu.github.io',
            owner: 'Sun-JunYu',
            clientID: 'fdde0a08ff8a1aa424a1',
            clientSecret: '7db7f7776e49fd7398aa0ce51caa9153b45597a5',
            admin: ["Sun-JunYu"],
            createIssueManually: true,
            distractionFreeMode: false,
            perPage: 10,
            pagerDirection: 'last',
            
            
            enableHotKey: true,
            isLocked: false
        })
        gitalk.render('comment-container')</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1 is-sticky"><div class="card widget toc-scroll" id="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="is-flex" id="toc-item-一-基础" href="#一-基础"><span>一.基础</span></a><ul class="menu-list"><li><a class="is-flex" id="toc-item-1-虚拟环境" href="#1-虚拟环境"><span>1.虚拟环境</span></a></li><li><a class="is-flex" id="toc-item-2-什么是爬虫" href="#2-什么是爬虫"><span>2.什么是爬虫</span></a></li><li><a class="is-flex" id="toc-item-3-爬虫的用途" href="#3-爬虫的用途"><span>3.爬虫的用途</span></a></li><li><a class="is-flex" id="toc-item-4-爬虫数据的来源" href="#4-爬虫数据的来源"><span>4.爬虫数据的来源</span></a></li><li><a class="is-flex" id="toc-item-5-爬虫的分类" href="#5-爬虫的分类"><span>5.爬虫的分类</span></a></li><li><a class="is-flex" id="toc-item-6-B-S架构和C-S架构" href="#6-B-S架构和C-S架构"><span>6.B&amp;#x2F;S架构和C&amp;#x2F;S架构</span></a></li><li><a class="is-flex" id="toc-item-7-PHP不适合做爬虫" href="#7-PHP不适合做爬虫"><span>7.PHP不适合做爬虫</span></a></li><li><a class="is-flex" id="toc-item-8-爬虫语言" href="#8-爬虫语言"><span>8.爬虫语言</span></a></li><li><a class="is-flex" id="toc-item-9-robots协议" href="#9-robots协议"><span>9.robots协议</span></a></li><li><a class="is-flex" id="toc-item-10-网络协议" href="#10-网络协议"><span>10.网络协议</span></a></li><li><a class="is-flex" id="toc-item-11-第一次请求" href="#11-第一次请求"><span>11. 第一次请求</span></a></li><li><a class="is-flex" id="toc-item-12-JSON爬取和JSON的一些简单的解析规则" href="#12-JSON爬取和JSON的一些简单的解析规则"><span>12.JSON爬取和JSON的一些简单的解析规则</span></a></li></ul></li><li><a class="is-flex" id="toc-item-二-爬虫的准备工作和爬虫的一些基础" href="#二-爬虫的准备工作和爬虫的一些基础"><span>二.爬虫的准备工作和爬虫的一些基础</span></a><ul class="menu-list"><li><a class="is-flex" id="toc-item-1-安装" href="#1-安装"><span>1. 安装</span></a></li><li><a class="is-flex" id="toc-item-2-第一个简单的爬虫" href="#2-第一个简单的爬虫"><span>2.第一个简单的爬虫</span></a></li><li><a class="is-flex" id="toc-item-3-爬虫的一些语法" href="#3-爬虫的一些语法"><span>3.爬虫的一些语法</span></a></li><li><a class="is-flex" id="toc-item-5-requests的高级应用" href="#5-requests的高级应用"><span>5.requests的高级应用</span></a></li><li><a class="is-flex" id="toc-item-6-利用urllib-request中的urlretrieve爬取图片" href="#6-利用urllib-request中的urlretrieve爬取图片"><span>6.利用urllib.request中的urlretrieve爬取图片</span></a></li><li><a class="is-flex" id="toc-item-7-正则" href="#7-正则"><span>7.正则</span></a></li><li><a class="is-flex" id="toc-item-8-xpath的语法" href="#8-xpath的语法"><span>8.xpath的语法</span></a></li><li><a class="is-flex" id="toc-item-9-BeautifulSoup库使用" href="#9-BeautifulSoup库使用"><span>9.BeautifulSoup库使用</span></a></li><li><a class="is-flex" id="toc-item-10-CSV写入文件和JSON写入" href="#10-CSV写入文件和JSON写入"><span>10.CSV写入文件和JSON写入</span></a></li><li><a class="is-flex" id="toc-item-11-selenium介绍" href="#11-selenium介绍"><span>11.selenium介绍</span></a></li><li><a class="is-flex" id="toc-item-11-雪球网站" href="#11-雪球网站"><span>11.雪球网站</span></a></li><li><a class="is-flex" id="toc-item-12-模拟QQ空间登陆-没有涉及图片滑动解锁" href="#12-模拟QQ空间登陆-没有涉及图片滑动解锁"><span>12.模拟QQ空间登陆(没有涉及图片滑动解锁)</span></a></li><li><a class="is-flex" id="toc-item-13-百度图片爬取-利用selenium-辉夜大小姐）" href="#13-百度图片爬取-利用selenium-辉夜大小姐）"><span>13.百度图片爬取,利用selenium (辉夜大小姐）</span></a></li><li><a class="is-flex" id="toc-item-14-谷歌无头浏览器" href="#14-谷歌无头浏览器"><span>14.谷歌无头浏览器</span></a></li></ul></li><li><a class="is-flex" id="toc-item-三-多线程爬虫" href="#三-多线程爬虫"><span>三.多线程爬虫</span></a><ul class="menu-list"><li><a class="is-flex" id="toc-item-1-并发和并行" href="#1-并发和并行"><span>1.并发和并行</span></a></li><li><a class="is-flex" id="toc-item-2-爬虫架构图" href="#2-爬虫架构图"><span>2.爬虫架构图</span></a></li><li><a class="is-flex" id="toc-item-3-多线程爬虫实例-存储的是csv格式" href="#3-多线程爬虫实例-存储的是csv格式"><span>3.多线程爬虫实例(存储的是csv格式)</span></a></li><li><a class="is-flex" id="toc-item-4-多线程爬取辽宁工程" href="#4-多线程爬取辽宁工程"><span>4.多线程爬取辽宁工程</span></a></li></ul></li><li><a class="is-flex" id="toc-item-四-第三方打码平台-超级鹰" href="#四-第三方打码平台-超级鹰"><span>四.第三方打码平台(超级鹰)</span></a><ul class="menu-list"><li><a class="is-flex" id="toc-item-1-打码平台代码" href="#1-打码平台代码"><span>1.打码平台代码</span></a></li></ul></li><li><a class="is-flex" id="toc-item-五-反爬" href="#五-反爬"><span>五.反爬</span></a><ul class="menu-list"><li><a class="is-flex" id="toc-item-1-selenium自动化操作中检测到security-check问题-cookie时效性差的问题" href="#1-selenium自动化操作中检测到security-check问题-cookie时效性差的问题"><span>1. selenium自动化操作中检测到security-check问题(cookie时效性差的问题)</span></a></li></ul></li><li><a class="is-flex" id="toc-item-六-Scrapy框架" href="#六-Scrapy框架"><span>六.Scrapy框架</span></a><ul class="menu-list"><li><a class="is-flex" id="toc-item-1-安装-1" href="#1-安装-1"><span>1.安装</span></a></li><li><a class="is-flex" id="toc-item-2-五大核心组件与数据流向" href="#2-五大核心组件与数据流向"><span>2.五大核心组件与数据流向</span></a></li><li><a class="is-flex" id="toc-item-3-创建一个项目" href="#3-创建一个项目"><span>3.创建一个项目</span></a></li><li><a class="is-flex" id="toc-item-4-获取详情页" href="#4-获取详情页"><span>4.获取详情页</span></a></li><li><a class="is-flex" id="toc-item-5-动态获取数据scrapy框架和selenium框架结合使用-网易新闻" href="#5-动态获取数据scrapy框架和selenium框架结合使用-网易新闻"><span>5.动态获取数据scrapy框架和selenium框架结合使用(网易新闻)</span></a></li><li><a class="is-flex" id="toc-item-6-数据持久化" href="#6-数据持久化"><span>6.数据持久化</span></a></li><li><a class="is-flex" id="toc-item-7-数据持久化的MongoDB的正规写法-管道" href="#7-数据持久化的MongoDB的正规写法-管道"><span>7.数据持久化的MongoDB的正规写法(管道)</span></a></li><li><a class="is-flex" id="toc-item-9-全栈数据爬取" href="#9-全栈数据爬取"><span>9.全栈数据爬取</span></a></li><li><a class="is-flex" id="toc-item-10-框架封装cookie" href="#10-框架封装cookie"><span>10.框架封装cookie</span></a></li></ul></li><li><a class="is-flex" id="toc-item-七-中间件" href="#七-中间件"><span>七.中间件</span></a><ul class="menu-list"><li><a class="is-flex" id="toc-item-1-UA池" href="#1-UA池"><span>1.UA池</span></a></li><li><a class="is-flex" id="toc-item-2-拦截之后更换IP" href="#2-拦截之后更换IP"><span>2.拦截之后更换IP</span></a></li><li><a class="is-flex" id="toc-item-3-拦截之后更换UA" href="#3-拦截之后更换UA"><span>3.拦截之后更换UA</span></a></li></ul></li><li><a class="is-flex" id="toc-item-八-增量式爬虫和深度爬虫" href="#八-增量式爬虫和深度爬虫"><span>八.增量式爬虫和深度爬虫</span></a><ul class="menu-list"><li><a class="is-flex" id="toc-item-1-增量式爬虫" href="#1-增量式爬虫"><span>1.增量式爬虫</span></a></li></ul></li></ul></div></div><script type="text/javascript" async>
        $(document).ready(function () { //参考自 https://github.com/ppoffice/hexo-theme-icarus/pull/616/files
            var observerTopMargin;
            var scrollObserver;
            var headerElems = $(".headerlink");
            var activeTocItem;
        
            function initIntersectionObserver(docHeight) {
                observerTopMargin = docHeight;
                scrollObserver = new IntersectionObserver(scrollCallBack,
                    {
                        root: null,  // viewpoint
                        rootMargin: docHeight + "px 0px -80% 0px"  // cover top 30% of viewport to the top of document
                    })
            }
        
            function scrollCallBack(entries, observer) {
                if ($(window).scrollTop() > observerTopMargin * 0.7) { 
                    // User somehow scroll to 70% of observerTopMargin (which is inited as 200% document height)
                    // Observer top margin need to extend to cover all the space to the top of the document
                    initIntersectionObserver(observerTopMargin * 2)
                    observer.disconnect();
                    return;
                }
                let toActive;
                if (entries[0].intersectionRatio == 1) {  // enter viewed area
                    let entry = entries.reduce((u, v) => (u.target.toc_id > v.target.toc_id ? u : v));  // get the lowest item
                    toActive = $("#toc-item-" + $(entry.target).attr("href").substr(1));
                } else {
                    let entry = entries.reduce((u, v) => (u.target.toc_id < v.target.toc_id ? u : v));  // get the highest item
                    let idx = Math.max(entry.target.toc_id - 1, 0);
                    toActive = $("#toc-item-" + $(headerElems[idx]).attr("href").substr(1));
                }
                if (activeTocItem) activeTocItem.removeClass("is-current");
                activeTocItem = toActive
                activeTocItem.addClass("is-current");
            }
        
            initIntersectionObserver($(document).height() * 2);
            headerElems.each(function (index, obj) {
                obj.toc_id = index;
                scrollObserver.observe(obj);
            })
        });</script></div><div class="card widget"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="is-rounded" src="http://q92ekyelb.bkt.clouddn.com/xsy.jpg" alt="孙军雨的技术博客"></figure><p class="title is-size-4 is-block line-height-inherit">孙军雨的技术博客</p><p class="is-size-6 is-block">总得有点追求，别太安于现状</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>中国</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">16</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">10</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">12</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://weibo.com/5814038760/profile?rightmod=1&amp;wvr=6&amp;mod=personnumber&amp;is_all=1" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-white is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/Sun-JunYu"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-white is-marginless" target="_blank" rel="noopener" title="Weibo" href="https://weibo.com/5814038760/profile?rightmod=1&amp;wvr=6&amp;mod=personnumber&amp;is_all=1"><i class="fab fa-weibo"></i></a><a class="level-item button is-transparent is-white is-marginless" target="_blank" rel="noopener" title="Email" href="mailto:sunjun_yu@163.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-white is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div><div><hr><p id="hitokoto">:D 一言句子获取中...</p><script type="text/javascript" defer>function getYiyan(){
                                $.getJSON("https://v1.hitokoto.cn/", function (data) {
                                if(data){
                                    $('#hitokoto').html("");
                                    $('#hitokoto').append("<strong style='color: #3273dc;'>"+data.hitokoto+"</strong>"+
                                    "<p>"+"来源《"+data.from+"》</p><p>提供者-"+data.creator+"</p>");
                                }});}
                                $(function (){getYiyan();$('#hitokoto').click(function(){getYiyan();})});</script></div></div></div><!--!--><div class="card widget"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content size-small"><p><time dateTime="2020-04-21T10:43:15.696Z">2020-04-21</time></p><p class="title is-6"><a class="link-muted" href="/posts/4a17b156/">Hello World</a></p><p class="is-uppercase"></p></div></article><article class="media"><a class="media-left" href="/posts/468ef951/"><p class="image is-64x64"><img class="thumbnail" src="http://q92ekyelb.bkt.clouddn.com/%E5%A8%9C%E7%BE%8E.jpg" alt="vue基础语法"></p></a><div class="media-content size-small"><p><time dateTime="2020-03-06T16:00:00.000Z">2020-03-07</time></p><p class="title is-6"><a class="link-muted" href="/posts/468ef951/">vue基础语法</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/vue/">vue</a></p></div></article><article class="media"><a class="media-left" href="/posts/468ef90b/"><p class="image is-64x64"><img class="thumbnail" src="https://cdn.jsdelivr.net/gh/removeif/blog_image/img/2019/20191221090606.png" alt="关于爬虫自己所了解的功能梳理"></p></a><div class="media-content size-small"><p><time dateTime="2020-03-06T16:00:00.000Z">2020-03-07</time></p><p class="title is-6"><a class="link-muted" href="/posts/468ef90b/">关于爬虫自己所了解的功能梳理</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a></p></div></article><article class="media"><a class="media-left" href="/posts/7eb35566/"><p class="image is-64x64"><img class="thumbnail" src="https://cdn.jsdelivr.net/gh/removeif/blog_image/img/2019/20191221090606.png" alt="git的基本操作"></p></a><div class="media-content size-small"><p><time dateTime="2020-03-04T16:00:00.000Z">2020-03-05</time></p><p class="title is-6"><a class="link-muted" href="/posts/7eb35566/">git的基本操作</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/git/">git</a></p></div></article><article class="media"><a class="media-left" href="/posts/b9330e24/"><p class="image is-64x64"><img class="thumbnail" src="https://cdn.jsdelivr.net/gh/removeif/blog_image/img/2019/20191221090606.png" alt="三方登录"></p></a><div class="media-content size-small"><p><time dateTime="2020-03-04T16:00:00.000Z">2020-03-05</time></p><p class="title is-6"><a class="link-muted" href="/posts/b9330e24/">三方登录</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E4%B8%89%E6%96%B9%E7%99%BB%E5%BD%95/">三方登录</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/git/"><span class="level-start"><span class="level-item">git</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/private/"><span class="level-start"><span class="level-item">private</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/python/"><span class="level-start"><span class="level-item">python</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/python/vue/"><span class="level-start"><span class="level-item">vue</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/python/vue/%E4%B8%83%E7%89%9B%E4%BA%91/"><span class="level-start"><span class="level-item">七牛云</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/vue/"><span class="level-start"><span class="level-item">vue</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E4%B8%89%E6%96%B9%E7%99%BB%E5%BD%95/"><span class="level-start"><span class="level-item">三方登录</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E6%97%A5%E8%AE%B0/"><span class="level-start"><span class="level-item">日记</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E6%B3%95%E5%BE%8B/"><span class="level-start"><span class="level-item">法律</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E7%88%AC%E8%99%AB/"><span class="level-start"><span class="level-item">爬虫</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><a class="level is-mobile is-marginless" href="/categories/"><span class="level-start"><span class="level-item">查看全部&gt;&gt;</span></span></a></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2020/04/"><span class="level-start"><span class="level-item">四月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/03/"><span class="level-start"><span class="level-item">三月 2020</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/02/"><span class="level-start"><span class="level-item">二月 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/01/"><span class="level-start"><span class="level-item">一月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2019/10/"><span class="level-start"><span class="level-item">十月 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><a class="level is-mobile is-marginless" href="/archives/"><span class="level-start"><span class="level-item">查看全部&gt;&gt;</span></span></a></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/%E6%B3%95%E5%BE%8B/"><span class="tag">法律</span><span class="tag is-grey-lightest">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python/"><span class="tag">python</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python%E5%9F%BA%E7%A1%80/"><span class="tag">python基础</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/vue/"><span class="tag">vue</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%B8%83%E7%89%9B%E4%BA%91/"><span class="tag">七牛云</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/vue%E5%9F%BA%E7%A1%80/"><span class="tag">vue基础</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%88%AC%E8%99%AB/"><span class="tag">爬虫</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%B8%89%E6%96%B9%E7%99%BB%E5%BD%95/"><span class="tag">三方登录</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8A%A0%E5%AF%86%E6%96%87%E7%AB%A0/"><span class="tag">加密文章</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%97%A5%E8%AE%B0/"><span class="tag">日记</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/django/"><span class="tag">django</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/git/"><span class="tag">git</span><span class="tag is-grey-lightest">1</span></a></div></div></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">订阅更新</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=removeifFeedsId&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="removeifFeedsId" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button is-primary" type="submit" value="订阅"></div></div><p class="help">输入邮箱开始订阅，更博后邮件通知！</p></form></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="http://q92ekyelb.bkt.clouddn.com/authon.jpg" alt="孙军雨的技术博客" height="28"></a><p class="size-small"><span>&copy; 2020 孙军雨的技术博客</span>  Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank">Icarus</a>,Modify by <a href="https://github.com/removeif" target="_blank">removeif</a> <br>© 版权说明：[本网站所有内容均收集于互联网或自己创作,<br>    方便于网友与自己学习交流，如有侵权，请<a href="/message" target="_blank">留言</a>，立即处理]<br><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/removeif/removeif.github.io@v2.2.1/js/statistics.js"></script><span id="statistic-times">网站运行时间统计加载中...</span><br><div class="size-small"><span id="busuanzi_container_site_uv">❤️感谢<strong> <span id="busuanzi_value_site_uv">99+</span> </strong></span>小伙伴的<strong> <span id="busuanzi_value_site_pv">99+</span> </strong>次光临，查看💐<a href="https://github.com/removeif/hexo-theme-amazing" target="_blank">主题源码</a>！❤️</div></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Sun-JunYu"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            site: {
                url: 'https://Sun-JunYu.github.io',
                external_link: {"enable":true,"exclude":[]}
            },
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script src="https://cdn.jsdelivr.net/gh/removeif/removeif.github.io@v2.2.1/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="https://cdn.jsdelivr.net/gh/removeif/removeif.github.io@v2.2.1/js/back-to-top.js" defer></script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/gh/removeif/removeif.github.io@v2.2.1/js/gallery.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/gh/removeif/removeif.github.io@v2.2.1/js/main.js" defer></script><script src="https://cdn.jsdelivr.net/gh/removeif/removeif.github.io@v2.2.1/js/comment-issue-data.js" defer></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/removeif/removeif.github.io@v2.2.1/css/insight.css"><div class="searchbox ins-search"><div class="searchbox-container ins-search-container"><div class="searchbox-input-wrapper"><input class="searchbox-input ins-search-input" type="text" placeholder="想要查找什么..."><span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span></div><div class="searchbox-result-wrapper ins-section-wrapper"><div class="ins-section-container"></div></div></div></div><script>(function (window) {
            var INSIGHT_CONFIG = {
                TRANSLATION: {
                    POSTS: '文章',
                    PAGES: '页面',
                    CATEGORIES: '分类',
                    TAGS: '标签',
                    UNTITLED: '(无标题)',
                },
                CONTENT_URL: 'https://cdn.jsdelivr.net/gh/removeif/removeif.github.io@v2.2.1/content.json',
            };
            window.INSIGHT_CONFIG = INSIGHT_CONFIG;
        })(window);</script><script src="https://cdn.jsdelivr.net/gh/removeif/removeif.github.io@v2.2.1/js/insight.js" defer></script></body></html>